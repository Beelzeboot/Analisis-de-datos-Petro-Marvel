# -*- coding: utf-8 -*-
"""TPFINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iEWijNQ5vHtvFUahu3Ui1LT3nhL5TSUi

#TP FINAL

# Ignacio Aprile

**Ejercicio 1**\
Buscar un dataset (Que no sea los trabajados en clases)

**Ejercicio 2**\
Realizar una introduccion al dataset de que se trata,definir sus variables (Diccionario de datos)

**Ejercicio 3**\
Identificar el tipo de variable,decide justificando su respuesta.

**Ejercicio 4**\
Detectar Valores Ausente  y Valores Atipicos .Decidir si eliminarlos y el por que de la eleccion.

**Ejercicio 5**\
Realizar un analisis univariado y en base a esos graficos,sacar conclusiones.

**Ejercicio 6**\
Realizar analisis de matriz corelacion y explicar que variable estan correlacionadas\

**Ejercicio 7**\
Sobre el Dataset Elegido  explique si se puede reducir las dimensiones  y que representa esas  nuevas variables.

#PETROLEO DATAFRAME

Buscar un dataset (Que no sea los trabajados en clases)

Link Archivo compartido Petroleo.csv: https://drive.google.com/file/d/1o-J4V1zGblgCLngTEUgsBAi_mAI0Sp0a/view?usp=sharing

Link Archivo compartido MarvelComics.csv: https://drive.google.com/file/d/1Cacv8i8OrRr14Ohox6Dbx2iI3whDBYx2/view?usp=sharing
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd


from google.colab import drive
drive.mount('/content/drive')
#ppnc_pd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CSV/produccin-de-pozos-de-gas-y-petrleo-no-convencional.csv')
ppnc_pd = pd.read_csv('/content/drive/MyDrive/CSV/produccin-de-pozos-de-gas-y-petrleo-no-convencional.csv')

"""Realizar una introduccion al dataset de que se trata,definir sus variables (Diccionario de datos)

"""

print(ppnc_pd.describe())
ppnc_pd.shape

"""El dataset contiene el detalle mensual de producción por pozo, de Petróleo [m3], gas en [Miles de m3] y agua en [m3].

Se indican los siguientes campos:


    idempresa (string): Identificador de la empresa operadora del pozo

    anio (integer): Año informado. Tipo de dato discreto

    mes (integer): Mes informado. Tipo de dato discreto

    idpozo (integer): Identificador del pozo por formación productiva. Tipo de dato categórico

    prod_pet (number): Producción de petróleo (en m3). Tipo de dato numérico continuo

    prod_gas (number): Producción de gas (en miles de m3). Tipo de dato numérico continuo

    prod_agua (number): Producción de agua (en m3). Tipo de dato numérico continuo

    iny_agua (number): Inyección de agua (en m3). Tipo de dato numérico continuo

    iny_gas (number): Inyección de gas (en miles de m3). Tipo de dato numérico continuo

    iny_co2 (number): Inyección de CO2 (en miles de m3). Tipo de dato numérico continuo

    iny_otro(number): Inyección de otros productos (en m3). Tipo de dato numérico continuo

    tef (number): Tiempo efectivo de funcionamiento del pozo. Expresado en días. Tipo de dato discreto

    vida_util (number): Vida útil estimada del pozo. Tipo de dato discreto

    tipoextraccion (string): Tipo de extracción (bombeo, surgente, plunger lift, sin sistema, Electrosumergible) de los fluidos del pozo. Dato categórico

    tipoestado (string): Estado en el que se encuentra el pozo. Dato categórico

    tipopozo (string): Tipo de pozo (petrolífero o gasífero) Dato categórico

    observaciones(string): Observaciones

    fechaingreso (date-time): Fecha de carga de los datos. Dato categórico

    rectificado (boolean): Los datos fueron rectificados por el operador. Dato categórico

    habilitado (boolean): Registro habilitado para su publicación. Dato categórico

    idusuario (integer): Identificador del usuario de carga de los datos. Dato categórico

    empresa (string): Nombre de la empresa operadora. Dato categórico

    sigla (string): Identificador único de la boca del pozo. Dato categórico

    formprod (string): Identificador de la formación productiva. Dato categórico

    profundidad (number): Profundidad del pozo. Dato numérico continuo

    formacion (string): Formación productiva. Dato categórico

    idareapermisoconcesion (string): Identificador del área, permiso o concesión. Dato categórico

    areapermisoconcesion (string): Nombre de área, permiso o concesión. Dato categórico

    idareayacimiento (string): Identificador del Yacimiento. Dato categórico

    areayacimiento (string): Nombre del yacimiento. Dato categórico

    cuenca (string): Nombre de la cuenca sedimentaria. Dato categórico

    provincia (string): Nombre de la provincia. Dato categórico
    
    coordenadax, coordenaday: coordenadas x e y de ubicación del pozo, expresadas según proyección WGS84 en coordenadas geográficas. Dato numérico continuo

    tipo_de_recurso (string): Tipo de recurso. Para el caso es NO CONVENCIONAL Dato categórico

    proyecto (string): Nombre del proyecto asociado al pozo. Dato categórico

    clasificacion (string): Clasificacion del pozo (Explotación / Exploración). Dato categórico

    subclasificacion (string): Subclasificacion del pozo (Avanzada, Exploración, En Desarrollo) Tipo de dato categórico

    sub_tipo_recurso (string): Tipo de Recurso hidrocarburífero (Tight o Shale) Dato categórico

    fecha_data: Fecha de los datos cargados. Dato categórico

Identificar el tipo de variable,decide justificando su respuesta

Las variables categóricas de la base de datos son: idempresa, idpozo, tipoextraccion, tipoestado, tipopozo, observaciones, rectificado, habilitado,  idusuario, empresa, sigla, formacion,  idareapermisoconcesion,areapermisoconcesion, idareayacimiento, areayacimiento, cuenca, provincia, tipo_de_recurso, proyecto, clasificacion, subclasificacion y sub_tipo_recurso.

En el caso de rectificado y habilitado son datos categóricos de tipo booleano

Estos datos definen categorías y permiten clasificar los mismos por medio de valores fijos asociados a una cualidad o categoría concreta.

Los datos numéricos discretos del dataset son: anio y mes. Estos datos son numéricos pero no continuos, dado que no puede tomar otro valor entre dos consecutivos.

Los datos numéricos continuos son:  prod_pet, prod_gas, prod_agua, iny_agua, iny_gas, iny_co2, iny_otro, tef y profundidad. Estos valores son numéricos continuos porque, en un intervalo cualquiera, existen siempre otros valores intermedios posibles.

También existen datos de fecha y hora, que en algunos textos se toman como continuos en otros como datos discretos. En este trabajo se toman como fecha y hora continuos. En el caso estas variables son: fechaingreso y fecha_data

Por último, existe una columna vida_util, de tipo numérico continuo, pero carente de datos en la base analizada

Detectar Valores Ausente  y Valores Atipicos .Decidir si eliminarlos y el por que de la eleccion.
"""

# Detectar datos nulos
nul=ppnc_pd.isnull().sum()
print('los nulos son',nul)

"""Se observa que las columnas vida_util y observaciones carecen de valor o el valor asignado carece de relevancia a los fines de este informe. Por lo que las mismas serán eliminadas del Dataset.

Respecto de los valores nulos en tipoextraccion, tipoestado, y tipopozo, que ascienden a 539 filas, representando el 0.2% de los datos, es decir, que carecen de incidencia en los valores finales y, tratándose de variables categóricas, las mismas se reemplazarán por la moda

Esta misma tesitura se adopta en las variables clasificacion y subclasificación, cuyos valores nulos ascienden a 778 y sub_tipo_recurso que asciende a 308, por idénticas razones.

"""

# Eliminar columnas con datos nulos
columnas_a_eliminar = ['vida_util', 'observaciones']

# Eliminar las columnas
ppnc_pd = ppnc_pd.drop(columns=columnas_a_eliminar)

# La columna tipo_de_recurso posee un valor único para todo el dataset, por lo que la misma puede eliminarse
valores_unicos = ppnc_pd['tipo_de_recurso'].unique()
print("Valores únicos:", valores_unicos)

# La columna iny_co2 tiene un valor único igual a "0" por lo que se elimina del dataframe
valores_unicos_iny_co2 =ppnc_pd['iny_co2'].unique()
print("Valores únicos iny_co2:", valores_unicos_iny_co2)

# La columna iny_otro tiene un valor único igual a "0" por lo que se elimina del dataframe
valores_unicos_iny_otro =ppnc_pd['iny_otro'].unique()
print("Valores únicos iny_otro:", valores_unicos_iny_otro)

# Elimnar columna tipo_de_recurso
ppnc_pd = ppnc_pd.drop(columns='tipo_de_recurso')

# Eliminar columna iny_co2
ppnc_pd = ppnc_pd.drop(columns='iny_co2')

# Eliminar columna iny_otro
ppnc_pd = ppnc_pd.drop(columns='iny_otro')

ppnc_pd.head()

# Modificar datos nulos por la moda

#Buscamos todas las variables categóricas
tipoextraccion_mas_frecuente = ppnc_pd['tipoextraccion'].mode()[0]
tipoestado_mas_frecuente = ppnc_pd['tipoestado'].mode()[0]
tipopozo_mas_frecuente = ppnc_pd['tipopozo'].mode()[0]
clasificacion_mas_frecuente = ppnc_pd['clasificacion'].mode()[0]
subclasificacion_mas_frecuente = ppnc_pd['subclasificacion'].mode()[0]
subtiporecurso_mas_frecuente = ppnc_pd['sub_tipo_recurso'].mode()[0]

#Reemplazamos los valores en el nuestro DataSet
ppnc_pd['tipoextraccion'].fillna(tipoextraccion_mas_frecuente, inplace=True)
ppnc_pd['tipoestado'].fillna(tipoestado_mas_frecuente, inplace=True)
ppnc_pd['tipopozo'].fillna(tipopozo_mas_frecuente, inplace=True)
ppnc_pd['clasificacion'].fillna(clasificacion_mas_frecuente, inplace=True)
ppnc_pd['subclasificacion'].fillna(subclasificacion_mas_frecuente, inplace=True)
ppnc_pd['sub_tipo_recurso'].fillna(subtiporecurso_mas_frecuente, inplace=True)

# Ver si se modificaron los nulos
nul=ppnc_pd.isnull().sum()
print('Comprobar la normalización de datos nulos\n', nul)

# Detectar valores atípicos
import matplotlib.pyplot as plt

# dado que no revisten interés los valores atípicos de las columnas idpozo e idusuario, se excluyen las mismas del ploteo. Igual tesitura se toma respecto a las coordenadas de ubicación de los pozos, las que, en todo caso, son motivo de otro análisis
columnas_a_analizar =['anio', 'mes', 'prod_pet', 'prod_gas', 'prod_agua', 'iny_agua', 'iny_gas', 'tef', 'profundidad']

plt.figure(figsize=(20, 10))
ppnc_pd[columnas_a_analizar].boxplot()
plt.show()

"""Se observa que posee valores atípicos en todas las columnas

En el caso de prod_pet y prod_gas indican el distinto rendimiento de los pozos

En el caso de prod_agua, dichos valores pueden indicar distintas cosas:

La cantidad de agua producida junto con el petróleo y el gas en un yacimiento de fracturación hidráulica proporciona información sobre las características y la eficacia del yacimiento, así como sobre la efectividad de los métodos de producción y fracturación utilizados.

Dicha variable puede indicar:

Mayor Cantidad de Agua:

Condiciones del Yacimiento: Una mayor producción de agua podría indicar que el yacimiento tiene una mayor saturación de agua, lo que a su vez podría sugerir que el yacimiento es más "húmedo" en términos de contenido de agua.

Comunicación con Acuíferos: Una mayor producción de agua podría indicar la comunicación entre el yacimiento y acuíferos subterráneos, lo que podría ser problemático en términos de contaminación de aguas subterráneas.

Efectividad de la Fracturación: En algunos casos, una mayor producción de agua podría indicar que la fracturación hidráulica ha abierto más vías de flujo de agua hacia el pozo. Esto podría ser una indicación de que la fracturación ha sido efectiva en liberar agua del yacimiento.

Recuperación Mejorada: En ciertos escenarios, una mayor cantidad de agua podría ser parte de un proceso de recuperación mejorada, donde se inyecta agua en el yacimiento para desplazar el petróleo y el gas hacia los pozos de producción.

Menor Cantidad de Agua:

Yacimiento Seco: Una menor producción de agua podría indicar que el yacimiento es relativamente "seco" en términos de contenido de agua, lo que podría ser una señal de un yacimiento con una mayor proporción de hidrocarburos.

Fracturación Limitada: Una menor producción de agua podría indicar que la fracturación hidráulica ha tenido un impacto limitado en el flujo de agua hacia el pozo, lo que podría ser una señal de que la fracturación no ha tenido un efecto significativo.
    
Eficacia de la Fracturación: En algunos casos, una menor producción de agua podría sugerir que la fracturación hidráulica no ha tenido éxito en abrir vías de flujo significativas en el yacimiento.
    
Mayor Concentración de Hidrocarburos: Una menor cantidad de agua en comparación con los hidrocarburos producidos podría indicar que el yacimiento contiene una proporción más alta de hidrocarburos en relación con el agua.

La interpretación de la producción de agua en un yacimiento es compleja y depende de muchos factores: características geológicas del yacimiento, métodos de producción utilizados, efectividad de la fracturación, entre otros.

Por lo tanto no se estima modificar los valores indicados

Por último, los valores atípicos en la inyección de gas iny_gas pueden ser indicativos de varias situaciones o problemas.

Problemas en la operación: Valores atípicos podrían indicar problemas operativos en el pozo o en el proceso de inyección. Esto podría incluir interrupciones en el flujo de gas, fallos en el equipo de inyección, o problemas en la presión o temperatura.

Variabilidad geológica: Las características geológicas del subsuelo, como la porosidad y permeabilidad, pueden variar de un lugar a otro. Valores atípicos en la inyección de gas podrían reflejar zonas con características inusuales en la formación geológica.

Contaminación o interferencia: La presencia de impurezas en el gas o interferencias externas podría afectar la inyección de gas y llevar a valores atípicos en los datos.

Cambios en la presión del yacimiento: La inyección de gas puede estar influenciada por la presión del yacimiento. Cambios repentinos o inesperados en la presión podrían generar valores atípicos.

Variabilidad en la fracturación: La forma en que se realizan las fracturas hidráulicas puede variar, lo que a su vez podría influir en la inyección de gas. Cambios en la dirección, tamaño o orientación de las fracturas pueden generar datos atípicos.

Reacciones químicas: Las interacciones químicas entre el gas y los fluidos subterráneos pueden afectar la inyección de gas. Estas reacciones podrían generar datos inusuales.

Problemas de medición: Valores atípicos también podrían deberse a problemas con los sistemas de medición. Es importante asegurarse de que los instrumentos de medición sean precisos y estén calibrados correctamente.

Por lo tanto tampoco este valor será modificado, correspondiendo analizar el mismo en concordancia con otros datos

Se impone analizar por pozo y no hacer un análisis general

"""

import pandas as pd
import matplotlib.pyplot as plt

# desde "ppnc_pd"

# 3. Agrupar los datos por 'idpozo'
grupos_por_pozo = ppnc_pd.groupby('idpozo')

# 4. Calcular la suma de producción para cada pozo
suma_produccion = grupos_por_pozo[['prod_pet', 'prod_gas', 'prod_agua', 'iny_agua', 'iny_gas']].sum()

# 5. Obtener información única por pozo
info_pozo = ppnc_pd.drop_duplicates('idpozo')[['idpozo', 'tef', 'tipoextraccion', 'tipopozo', 'clasificacion', 'subclasificacion', 'profundidad', 'idempresa', 'coordenadax', 'coordenaday']]

# 6. Combinar los resultados en un único DataFrame
resultado_df = suma_produccion.join(info_pozo.set_index('idpozo'), on='idpozo')

# 7. Reseteamos los indices del DataFrame
resultado_df = resultado_df.reset_index()
resultado_df.head()
resultado_df.shape

"""

Se advierte que muchos de los pozos tienen producción de gas y petróleo = 0

Estas filas resultan innecesarias en el análisis, por lo tanto se crean dos nuevos dataset con los pozos en producción y con los pozos sin producción para dividir el análisis
"""

# Filtrar los pozos con producción de gas o petróleo
pozos_con_produccion = resultado_df[(resultado_df['prod_pet'] > 0) | (resultado_df['prod_gas'] > 0)]
pozos_sin_produccion = resultado_df[(resultado_df['prod_pet'] == 0) & (resultado_df['prod_gas'] == 0)]

print(pozos_con_produccion)
print(pozos_sin_produccion)

#importar librerías
import pandas as sp
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Crear un DataFrame vacío para almacenar los resultados
resultados_por_empresa = pd.DataFrame(columns=['Empresa', 'Pozo', 'Prod total pet', 'Prod total gas', 'Prod total Agua', 'Iny agua', 'Iny gas', 'Tiempo Efect Func'])
estadistica_por_empresa = pd.DataFrame(columns=['Empresa', 'Pozo', 'Promedio Prod. Gas', 'Promedio Prod. Pet', 'Promedio_prod_agua', 'Max Prod. Pet', 'Max Prod. Gas', 'Max Prod agua', 'Min prod gas', 'Min prod pet', 'Min prod agua'])

# Crear grupos por empresa
grupos_por_empresa = pozos_con_produccion.groupby('idempresa')

# Realizar análisis en cada grupo
for pozo, grupo in grupos_por_empresa:
    # pozo es el valor de la columna 'ID_Pozo' para ese grupo
    # grupo es un DataFrame que contiene los datos de cada empresa

    # Realiza análisis específicos por empresa, para cada valor

    promedio_prod_gas = grupo['prod_gas'].mean()
    promedio_prod_pet = grupo['prod_pet'].mean()
    promedio_prod_agua = grupo['prod_agua'].mean()
    max_valor_pet = grupo['prod_gas'].max()
    max_valor_gas = grupo['prod_gas'].max()
    max_valor_agua = grupo['prod_agua'].max()
    min_valor_gas = grupo['prod_gas'].min()
    min_valor_pet = grupo['prod_pet'].min()
    min_valor_agua = grupo['prod_agua'].min()

    nuevo_resultado = pd.DataFrame({
        'Empresa': [grupo['idempresa'].iloc[0]],
        'Pozo': [grupo['idpozo'].iloc[0]],
        'Prod total pet': [grupo['prod_pet'].iloc[0]],
        'Prod total gas': [grupo['prod_gas'].iloc[0]],
        'Prod total Agua': [grupo['prod_agua'].iloc[0]],
        'Iny agua': [grupo['iny_agua'].iloc[0]],
        'Iny gas': [grupo['iny_gas'].iloc[0]],
        'Tiempo Efect Func': [grupo['tef'].iloc[0]]
    })

    resultados_por_empresa = pd.concat([resultados_por_empresa, nuevo_resultado], ignore_index=True)

print(resultados_por_empresa.head(25))
resultados_por_empresa.shape

#En resultados_por_empresa

#crear un excel para ver los resultados
nombre_archivo = 'resultados_empresa.xlsx'

# Guardar el DataFrame en un archivo Excel
resultados_por_empresa.to_excel(nombre_archivo, index=True)

from google.colab import files

# Especifica el nombre del archivo Excel
nombre_archivo = 'resultados_empresa.xlsx'

# Descarga el archivo desde Colab
files.download(nombre_archivo)

# En resultado_df
#crear un excel para ver los resultados
nombre_archivo = 'resultado_df.xlsx'

# Guardar el DataFrame en un archivo Excel
resultado_df.to_excel(nombre_archivo, index=True)

from google.colab import files

# Especifica el nombre del archivo Excel
nombre_archivo = 'resultado_df.xlsx'

# Descarga el archivo desde Colab
files.download(nombre_archivo)

# Análisis de Resultados por empresa

for empresa, grupo in grupos_por_empresa:
  plt.figure(figsize=(18, 10))
  plt.xticks(fontsize=9)  # Tamaño de fuente para las etiquetas en el eje x
  plt.yticks(fontsize=12)  # Tamaño de fuente para las etiquetas en el eje y
  plt.xlabel(f'Resultados por Empresa {empresa}', fontsize=14)
  plt.ylabel('Valor', fontsize=12)
  plt.title('Gráfica para la Empresa {}'.format(empresa), fontsize=16)
  pozos_con_produccion.boxplot()
  plt.grid()
  plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Análisis de Resultados por empresa

# Calcular la producción total de petróleo, gas y agua por empresa
petroleum_production = resultados_por_empresa.groupby('Empresa')['Prod total pet'].sum()
gas_production = resultados_por_empresa.groupby('Empresa')['Prod total gas'].sum()
agua_production = resultados_por_empresa.groupby('Empresa')['Prod total Agua'].sum()

# Crear un gráfico de barras combinado para petróleo, gas y agua por empresa
fig, ax = plt.subplots(figsize=(10, 6))

petroleum_production.plot(kind='bar', color='blue', position=0, width=0.3, label='Producción de Petróleo')
gas_production.plot(kind='bar', color='orange', position=1, width=0.3, label='Producción de Gas')
agua_production.plot(kind='bar', color='green', position=2, width=0.3, label='Producción de Agua')

ax.set_title('Producción de Petróleo, Gas y Agua por Empresa')
ax.set_xlabel('Empresa')
ax.set_ylabel('Producción')
ax.legend()

plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.show()

print(len(pozos_con_produccion['coordenadax']))
print(len(pozos_con_produccion['coordenaday']))

# Calcula la media de la columna 'prod_agua'
media_prod_agua = pozos_con_produccion['prod_agua'].mean()

# Filtra los pozos con 'prod_agua' igual o superior a la media
pozos_filtrados = pozos_con_produccion[pozos_con_produccion['prod_agua'] >= media_prod_agua]

coordenadas_list = list(zip(pozos_con_produccion['coordenaday'], pozos_con_produccion['coordenadax']))
print(coordenadas_list)

import random
import numpy as np
import folium

mapa = folium.Map(location=[sum([coord[0] for coord in coordenadas_list]) / len(coordenadas_list),
                             sum([coord[1] for coord in coordenadas_list]) / len(coordenadas_list)],
                  zoom_start=6)

# Crear un array con los colores posibles
colores_permitidos = ['cadetblue', 'lightgray', 'red', 'blue', 'darkgreen', 'lightblue', 'white', 'purple', 'pink', 'green', 'orange', 'beige', 'lightred', 'lightgreen', 'darkblue', 'darkpurple', 'black', 'darkred', 'gray']

# Crear un diccionario para asignar colores a cada empresa
colores_empresas = {}

# Recorrer las empresas únicas y asignar un color aleatorio a cada una
empresas_unicas = pozos_filtrados['idempresa'].unique()
for empresa in empresas_unicas:
    color = np.random.choice(colores_permitidos)
    colores_empresas[empresa] = color

# Agregar marcadores para cada coordenada de pozo
for index, row in pozos_filtrados.iterrows():
    coord = (row['coordenaday'], row['coordenadax'])
    empresa = row['idempresa']
    color_empresa = colores_empresas[empresa]
    popup_text = f"ID Empresa: {empresa}"
    folium.Marker(location=coord, icon=folium.Icon(color=color_empresa), popup=popup_text).add_to(mapa)

# Guardar el mapa interactivo en un archivo HTML
mapa.save('mapa_pozos_filtrados.html')

print('Usted puede descargar el mapa en /content/mapa_pozos_filtrados.html, y posicionarse sobre cada pozo para ver la empresa que explota el mismo')

# Calcula la media de la columna 'prod_agua'
media_prod_agua = pozos_con_produccion['prod_agua'].mean()

# Filtra los pozos con 'prod_agua' igual o superior a la media
pozos_filtrados = pozos_con_produccion[pozos_con_produccion['prod_agua'] >= media_prod_agua]

# Crear el mapa
mapa = folium.Map(location=[sum(pozos_con_produccion['coordenaday']) / len(pozos_con_produccion),
                             sum(pozos_con_produccion['coordenadax']) / len(pozos_con_produccion)],
                  zoom_start=6)

# Recorrer los pozos filtrados y agregar marcadores con colores según la producción de agua
for index, row in pozos_con_produccion.iterrows():
    coord = (row['coordenaday'], row['coordenadax'])
    color = 'blue' if row['prod_agua'] >= media_prod_agua else 'red'
    folium.Marker(location=coord, icon=folium.Icon(color=color)).add_to(mapa)

# Guardar el mapa interactivo en un archivo HTML
mapa.save('mapa_pozos_coloreados.html')
print('No se evidencia de la gráfica que exista una relación entre la cercanía del pozo al acuífero y la producción de agua')

# Crear el mapa
mapa = folium.Map(location=[sum(pozos_con_produccion['coordenaday']) / len(pozos_con_produccion),
                             sum(pozos_con_produccion['coordenadax']) / len(pozos_con_produccion)],
                  zoom_start=6)

# Recorrer los pozos filtrados y agregar marcadores con colores según la producción de agua
for index, row in pozos_con_produccion.iterrows():
    coord = (row['coordenaday'], row['coordenadax'])
    color = 'blue' if row['prod_agua'] >= media_prod_agua else 'red'
    folium.Marker(location=coord, icon=folium.Icon(color=color)).add_to(mapa)

# Guardar el mapa interactivo en un archivo HTML
mapa.save('mapa_pozos_coloreados.html')
print('''Usted puede descargar el mapa en /content/mapa_pozos_coloreados.html
En este mapa se distinguen con azul los pozos con producción de agua igual o mayor a la media y con rojo los restantes.
No se evidencia de la gráfica que exista una relación entre la cercanía del pozo al acuífero y la producción de agua''')

# BoxPlot de pozos con producción

plt.figure(figsize=(18, 10))  # Tamaño de la figura (ancho, alto)
plt.xticks(fontsize=9)  # Tamaño de fuente para las etiquetas en el eje x
plt.yticks(fontsize=12)  # Tamaño de fuente para las etiquetas en el eje y
plt.xlabel('Resultados por Empresa', fontsize=14)
plt.ylabel('Valor', fontsize=14)  # Cambia el tamaño de fuente para la etiqueta del eje y
pozos_con_produccion.boxplot()
plt.show()

# Regularizar outliers en resultados por empresa

columns_to_check =  ['Prod total pet', 'Prod total gas', 'Prod total Agua', 'Iny agua', 'Iny gas', 'Tiempo Efect Func']


def identificar_atipicos_iqr(dataframe, columns, factor):
    outliers = pd.DataFrame()
    lower_bounds = {}  # Diccionario para almacenar los límites inferiores por columna
    upper_bounds = {}  # Diccionario para almacenar los límites superiores por columna

    for col in columns:
        Q1 = dataframe[col].quantile(0.25)
        Q3 = dataframe[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - factor * IQR
        upper_bound = Q3 + factor * IQR

        outliers_col = dataframe[(dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)]
        outliers = pd.concat([outliers, outliers_col])

        lower_bounds[col] = lower_bound
        upper_bounds[col] = upper_bound

    return outliers, lower_bounds, upper_bounds

# Establecer las columnas a identificar valores atípicos

outliers_iqr, lower_bounds, upper_bounds = identificar_atipicos_iqr(resultados_por_empresa, columns_to_check, factor=1)

for col in resultados_por_empresa.columns:
    if (col != 'Empresa') and (col != 'Pozo'):
        for i in range(len(resultados_por_empresa)):
            if (resultados_por_empresa.loc[i, col] < lower_bounds[col]) or (resultados_por_empresa.loc[i, col] > upper_bounds[col]):
                resultados_por_empresa.loc[i, col] = resultados_por_empresa[col].mode()[0]

plt.figure(figsize=(15, 6))
resultados_por_empresa.boxplot() # para verificar que se corrigieran los atípicos
plt.show()

print('\nNo logran corregirse todos los atípicos del dataset')

pozos_con_produccion.columns.tolist

# Regularizar outliers en pozos con producción

# Establecer las columnas a identificar valores atípicos
columns_to_check = ['prod_pet', 'prod_gas', 'prod_agua', 'iny_agua', 'iny_gas', 'tef', 'profundidad']

outliers_iqr, lower_bounds, upper_bounds = identificar_atipicos_iqr(pozos_con_produccion, columns_to_check, factor=1)

for col in pozos_con_produccion.columns:
    if (col not in ['idpozo', 'tipoextraccion', 'tipopozo', 'clasificacion', 'subclasificacion', 'idempresa', 'coordenadax', 'coordenaday', 'coordenadas']):
        mask = (pozos_con_produccion[col] < lower_bounds[col]) | (pozos_con_produccion[col] > upper_bounds[col])
        pozos_con_produccion.loc[mask, col] = pozos_con_produccion[mask][col].apply(lambda x: pozos_con_produccion[col].mode()[0])

plt.figure(figsize=(15, 6))
pozos_con_produccion[columns_to_check].boxplot() # para verificar que se corrigieran los atípicos
plt.show()

print('\nNo logran corregirse todos los atípicos')

# Estandarización de datos
# se utiliza columns_to_check definida en el bloque anterior para limitar a las numéricas
columns_to_check =  ['Prod total pet', 'Prod total gas', 'Prod total Agua', 'Iny agua', 'Iny gas', 'Tiempo Efect Func']


# Crear un objeto StandardScaler
scaler = StandardScaler()

# Aplicar el escalado a las columnas numéricas y crear un nuevo DataFrame escalado
data_scaled = scaler.fit_transform(resultados_por_empresa[columns_to_check])
resultados_por_empresa_scaled = pd.DataFrame(data_scaled, columns=columns_to_check)

# Mostrar el DataFrame escalado
print(resultados_por_empresa_scaled)

# Análisis univariado con datos escalados 'Prod total gas', 'Prod total Agua', 'Iny agua', 'Iny gas', 'Tiempo Efect Func'
plt.figure(figsize=(18, 15))

plt.subplot(2, 3, 1)
resultados_por_empresa_scaled['Prod total pet'].plot(kind='hist')
plt.xlabel('Prod total pet')
plt.title('Histograma: Prod total pet')

plt.subplot(2, 3, 2)
resultados_por_empresa_scaled['Prod total gas'].plot(kind='hist')
plt.xlabel('Prod total gas')
plt.title('Histograma: Prod total gas')

plt.subplot(2, 3, 3)
resultados_por_empresa_scaled['Prod total Agua'].plot(kind='hist')
plt.xlabel('Prod total Agua')
plt.title('Histograma: Prod total Agua')

plt.subplot(2, 3, 4)
resultados_por_empresa_scaled['Iny agua'].plot(kind='hist')
plt.xlabel('Iny agua')
plt.title('Histograma: Iny agua')

plt.subplot(2, 3, 5)
resultados_por_empresa_scaled['Iny gas'].plot(kind='hist')
plt.xlabel('Iny gas')
plt.title('Histograma: Iny gas')

plt.subplot(2, 3, 6)
resultados_por_empresa_scaled['Tiempo Efect Func'].plot(kind='hist')
plt.xlabel('Tiempo Efect Func')
plt.title('Histograma: Tiempo Efect Func')

plt.tight_layout()
plt.show()

print('\nNo surgen observaciones del análisis univariado de resultados por empresa')

"""# Análisis sobre resultado_df (base de datos original)"""

#Librerias
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

"""Variables Numéricas"""

columnas_a_analizar_numéricas = ['prod_pet', 'prod_gas', 'prod_agua', 'iny_agua', 'iny_gas', 'tef', "profundidad"]


plt.figure(figsize=(12, 10))
for i, col in enumerate(columnas_a_analizar_numéricas, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(data=resultado_df , x=col)
    plt.title(f"Boxplot de {col}")
    plt.xticks(rotation=45)
    plt.tight_layout()

plt.show()

"""Variables Categóricas"""

#Para las columnas categóricas
columnas_a_analizar_categóricas = ["tipoextraccion", "tipopozo", "clasificacion", "subclasificacion",
                                   ]


for columna in columnas_a_analizar_categóricas:
    plt.figure(figsize=(18, 8))

    # Gráfico de barras
    plt.subplot(1, 2, 1)
    resultado_df[columna].value_counts().plot(kind='bar')
    plt.title(f'Gráfico de Barras: {columna}')
    plt.xlabel(columna)
    plt.ylabel("Frecuencia")
    plt.xticks(rotation=45)

    # Gráfico de pastel
    plt.subplot(1, 2, 2)
    resultado_df[columna].value_counts().plot(kind='pie', autopct='%1.f%%')
    plt.title(f'Gráfico de Pastel: {columna}')
    plt.ylabel("")

    plt.tight_layout()
    plt.show()

# Analisis del gráfico 1 tipoextraccion
total = resultado_df["tipoextraccion"].value_counts().sum()
particion = resultado_df["tipoextraccion"].value_counts()

resultado_tipoextraccion = (particion / total)*100
print(resultado_tipoextraccion)

"""# **ANALISIS DE CORRELACIONES Y MULTIVARIADO** sobre resultado_df"""

#ANALISIS MULTIVARIADO

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(resultado_df)
plt.suptitle("Petroleo y Gas")
plt.show()

#ANALISIS 3D EN DISPERCION DE GAS, PETROLEO Y AGUA

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(resultado_df['prod_pet'], resultado_df['prod_gas'], resultado_df['prod_agua'])
ax.set_xlabel('Produccion de Petroleo')
ax.set_ylabel('Producion de Gas')
ax.set_zlabel('Produccion de Agua')
plt.title('3D Dispercion del Petroleo y Gas')
plt.show()

#GRAFICA DE CORRELACION DE PETROLEO Y GAS

correlation_matrix = resultado_df.corr(numeric_only=True)
print(correlation_matrix)
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("GRAFICA PETROLEO GAS")
plt.show()

print('\nNo se advierte correlación entre las variables numéricas')
#EN ESTA MATRIZ SOLO VEMOS LAS COLUMNAS NUMERICAS, LAS CATEGORICAS NO PUEDEN SER EXPRESADAS EN ESTA MATRIZ

# GRAFICA DE CORRELACION REDUCIDA

columnas_extraccion = ['prod_gas', 'prod_pet', 'prod_agua']
extraccion_df = resultado_df[columnas_extraccion]
correlation_matrix = extraccion_df.corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="YlGnBu")
plt.title('Matriz de Correlación Reducida')
plt.show()

#CON LOS INDICADORES DE MATRIZ ANTERIOR PROCEDO A HACER UN MAS PEQUEÑA CON LOS VALORES QUE TIENDEN A 1

"""# ANALISIS DE DATOS CORRELACIONADOS

Segun las graficas anteriormente representadas podemos ver un claro ejemplo de la correlacion que existe entre la produccion de petroleo, gas y agua.

Esto se debe que segun los metodos aplicados para la extraccion de estos, principalmente de yacimientos subterraneos, mediante la perforacion en donde se destaca la ***Surgencia Natural y el Plunger Lift*** al adquirir el gas y el petroleo es comun que tambien se produzca agua que se encuentra en estos yacimientos anteriormente mencionados.

Por ende la cantidad de agua producida no necesariamente es proporcional a los otros dos productos extraido, como vemos en la grafica representadas, ya que la extraccion de gas es la mas alta seguida por el petroleo.

La relación entre la producción de gas, petróleo y agua puede variar según la ubicación del yacimiento, la geología del área y las características específicas de cada pozo.

Si vemos otras variables que se pueden sumar en un analisis mas profundo, la relacion de produccion de los tres productos extraidos puede variar a medida que envejecen y tambien puede ser influenciado por la tecnica implementada para la sustraccion de estos.

# ANALISIS SOBRE RESULTADOS POR EMPRESA CON DATOS ESCALADOS
Sólo numéricos
"""

resultados_por_empresa_scaled.columns.tolist()

columnas_a_analizar_numéricas = ['Prod total pet', 'Prod total gas', 'Prod total Agua', 'Iny agua', 'Iny gas', 'Tiempo Efect Func']


plt.figure(figsize=(12, 10))
for i, col in enumerate(columnas_a_analizar_numéricas, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(data=resultados_por_empresa_scaled, x=col)
    plt.title(f"Boxplot de {col}")
    plt.xticks(rotation=45)
    plt.tight_layout()

plt.show()

#ANALISIS MULTIVARIADO

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(resultados_por_empresa_scaled)
plt.suptitle("Petroleo, Gas y Agua")
plt.show()

#ANALISIS 3D EN DISPERCION DE GAS, PETROLEO Y AGUA

# columnas del df 'Prod total pet', 'Prod total gas', 'Prod total Agua', 'Iny agua', 'Iny gas', 'Tiempo Efect Func'

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(resultados_por_empresa_scaled['Prod total pet'], resultados_por_empresa_scaled['Prod total gas'], resultados_por_empresa_scaled['Prod total Agua'])
ax.set_xlabel('Produccion de Petroleo')
ax.set_ylabel('Producion de Gas')
ax.set_zlabel('Produccion de Agua')
plt.title('3D Dispercion del Petroleo Gas y Agua')
plt.show()

#GRAFICA DE CORRELACION DE PETROLEO Y GAS

correlation_matrix = resultados_por_empresa_scaled.corr(numeric_only=True)
print(correlation_matrix)
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("GRAFICA PETROLEO GAS")
plt.show()

print('''\nNo se advierte correlación entre las variables numéricas
Las variables inyección de agua e inyección de gas son ceros en la población delimitada por el dataset''')
#EN ESTA MATRIZ SOLO VEMOS LAS COLUMNAS NUMERICAS, LAS CATEGORICAS NO PUEDEN SER EXPRESADAS EN ESTA MATRIZ

# PCA
from sklearn.decomposition import PCA

# Extraer las características
X = resultados_por_empresa_scaled[['Prod total pet', 'Prod total gas', 'Prod total Agua']]

# Aplicar PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(X)

# Crear un nuevo DataFrame con los componentes principales
df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
#df_pca['variety'] = df['variety']

# Plot de los componentes principales
plt.figure(figsize=(10, 6))
plt.scatter(df_pca['PC1'], df_pca['PC2'])#, c=df_pca['variety'])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA: Componentes principales PC1 vs PC2')
plt.show()

print('''\nSurgiría una curva descendente -negativa- que relaciona las principales componentes entre producción total de petróleo,
producción total de gas y producción total de agua, a pesar de existir valores disperos fuera de la curva
Se advierte que, sin tomar en cuenta los valores dispersos, a medida que x aumenta y disminuye en una relación lineal''')

"""# ANALISIS SOBRE POZOS CON PRODUCCION
Sólo numéricos
"""

#ESCALAR LOS DATOS
pozos_con_produccion.columns.tolist()

# Estandarización de datos
# se utiliza columns_to_check definida en el bloque anterior para limitar a las numéricas
columns_to_check =  ['prod_pet', 'prod_gas', 'prod_agua', 'iny_agua', 'iny_gas', 'tef', 'profundidad', 'coordenadax', 'coordenaday']


# Crear un objeto StandardScaler
scaler = StandardScaler()

# Aplicar el escalado a las columnas numéricas y crear un nuevo DataFrame escalado
data_scaled = scaler.fit_transform(pozos_con_produccion[columns_to_check])
pozos_con_produccion_scaled = pd.DataFrame(data_scaled, columns=columns_to_check)

# Mostrar el DataFrame escalado
print(pozos_con_produccion_scaled)

# Análisis univariado con datos escalados 'prod_pet', 'prod_gas', 'prod_agua', 'iny_agua', 'iny_gas', 'tef', 'profundidad', 'coordenadax', 'coordenaday'
plt.figure(figsize=(27, 15))

plt.subplot(3, 3, 1)
pozos_con_produccion_scaled['prod_pet'].plot(kind='hist')
plt.xlabel('Prod total pet')
plt.title('Histograma: Prod total pet')

plt.subplot(3, 3, 2)
pozos_con_produccion_scaled['prod_gas'].plot(kind='hist')
plt.xlabel('Prod total gas')
plt.title('Histograma: Prod total gas')

plt.subplot(3, 3, 3)
pozos_con_produccion_scaled['prod_agua'].plot(kind='hist')
plt.xlabel('Prod total Agua')
plt.title('Histograma: Prod total Agua')

plt.subplot(3, 3, 4)
pozos_con_produccion_scaled['iny_agua'].plot(kind='hist')
plt.xlabel('Iny agua')
plt.title('Histograma: Iny agua')

plt.subplot(3, 3, 5)
pozos_con_produccion_scaled['iny_gas'].plot(kind='hist')
plt.xlabel('Iny gas')
plt.title('Histograma: Iny gas')

plt.subplot(3, 3, 6)
pozos_con_produccion_scaled['tef'].plot(kind='hist')
plt.xlabel('Tiempo Efect Func')
plt.title('Histograma: Tiempo Efect Func')

plt.subplot(3, 3, 7)
pozos_con_produccion_scaled['profundidad'].plot(kind='hist')
plt.xlabel('Profundidad')
plt.title('Histograma: Profundidad')

plt.subplot(3, 3, 8)
pozos_con_produccion_scaled['coordenadax'].plot(kind='hist')
plt.xlabel('coordenada x')
plt.title('Histograma: coordenada x')

plt.subplot(3, 3, 9)
pozos_con_produccion_scaled['coordenaday'].plot(kind='hist')
plt.xlabel('coordenada y')
plt.title('Histograma: coordenada y')

plt.tight_layout()
plt.show()

print('\nEn los pozos con producción con datos escalados surgiría una relación entre producción de petróleo, gas y agua')

#ANALISIS MULTIVARIADO

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(pozos_con_produccion_scaled)
plt.suptitle("Petroleo, Gas y Agua")
plt.show()

#ANALISIS 3D EN DISPERCION DE GAS, PETROLEO Y AGUA

# columnas del df 'prod_pet', 'prod_gas', 'prod_agua'

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(pozos_con_produccion_scaled['prod_pet'], pozos_con_produccion_scaled['prod_gas'], pozos_con_produccion_scaled['prod_agua'])
ax.set_xlabel('Produccion de Petroleo')
ax.set_ylabel('Producion de Gas')
ax.set_zlabel('Produccion de Agua')
plt.title('3D Dispercion del Petroleo Gas y Agua')
plt.show()

#GRAFICA DE CORRELACION DE PETROLEO Y GAS

correlation_matrix = pozos_con_produccion_scaled.corr(numeric_only=True)
print(correlation_matrix)
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("GRAFICA PETROLEO GAS")
plt.show()

print('\nNo se advierte correlación entre las variables numéricas')
#EN ESTA MATRIZ SOLO VEMOS LAS COLUMNAS NUMERICAS, LAS CATEGORICAS NO PUEDEN SER EXPRESADAS EN ESTA MATRIZ

# PCA
from sklearn.decomposition import PCA

# Extraer las características
X = pozos_con_produccion_scaled[['prod_pet', 'prod_gas', 'prod_agua']]

# Aplicar PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(X)

# Crear un nuevo DataFrame con los componentes principales
df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
#df_pca['variety'] = df['variety']

# Plot de los componentes principales
plt.figure(figsize=(10, 6))
plt.scatter(df_pca['PC1'], df_pca['PC2'])#, c=df_pca['variety'])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA: Componentes principales PC1 vs PC2')
plt.show()

print('''\nSe advierte una relación entre producción de petróleo, producción de gas y
producción de agua en el dataset reducido de pozos con producción escalado''')

"""# MARVEL DATAFRAME"""

# Primer consigna
import pandas as pd


from google.colab import drive
drive.mount('/content/drive')
MV_df = pd.read_csv('/content/drive/MyDrive/CSV/Marvel_Comics.csv')

MV_df.shape

MV_df.sample()

"""Importante resaltar el active_year del cuál desglozamos el añoactivo_comienzo y añoactivo_fin

POS PROCESAMIENTO DATAFRAME

DICCIONARIO DE DATOS (NO RECOMPILAR, en caso de error, recompilar todo el proyecto y al finalizar compilar este head)
"""

MV_df_limpio_final.head()

"""Comic_name: Variables cualititativa(categórica) . Nombre del Comic

Issue_title: Variables cualititativa(categórica). Titulo del número de Comic

Publish_date: Variables cualititativa(categórica). Fecha de publicación

issue_description: Variables cualititativa(categórica). Introducción al contenido del comic

penciler: Variables cualititativa(categórica). Dibujante del comic

writer: Variables cualititativa(categórica). Escritor del comic

cover_artis: Variables cualititativa(categórica). Artista de la portada del comic

imprint: Variables cualititativa(categórica). Editorial del comic

Format: Variables cualititativa(categórica). tipo de comic, ej:tapadura,comic, colección de historias.

Rating: Variables cualititativa(categórica). Calificación  del comic

Price: variable cuantitativa (numérica) continua. Precio del artículo ej: 3.99

Añoactivo_comienzo: variable cuantitativa (numérica) discreta. Año de comienzo de emision

Añoactivo_fin: variable cuantitativa (numérica) discreta. Año de fin de emisión

Días: variable cuantitativa (numérica) discreta. Cantidad de días en emisión

Vamos a procesar la información para contar con más datos numéricos, la columna "active_years", tiene diferentes formatos ej : (2016) , (2017 - 2018) , (2016 .
Vamos a separarlos en 2 columnas con los añosactivo_comienzo y añosactivo_fin
"""

import numpy as np

# Dividir la columna "active_years" en dos columnas de añosactivo , una para el inicio y otra para el fin de sus años activos
años_activo = MV_df["active_years"].str.extract(r'\((\d{4})(?: - (\d{4}))?\)')
MV_df["añoactivo_comienzo"] = años_activo[0].apply(lambda x: int(x) if pd.notnull(x) else None)


# Los años que se encuentren vacios/nulos son porque solo tiene rango de inicio, en la columna año fin se reemplazan por el año inicio, ya que se emitió solo ese año
MV_df["añoactivo_fin"] = años_activo[1].apply(lambda x: int(x) if pd.notnull(x) else None)# Rellenar con el valor de [0] si es NaN

# Llenar valores NaN en caso de que no haya coincidencias en el formato
MV_df["añoactivo_comienzo"].fillna(0, inplace=True)
MV_df["añoactivo_fin"].fillna(0, inplace=True)

# Eliminar la columna original "active_years"
MV_df.drop("active_years", axis=1, inplace=True)


# Cambio el formato a lower case
MV_df["Imprint"] = MV_df["Imprint"].str.lower()

MV_df.isnull().sum()

"""Al tener datos Nulos, estos se deben al procesamiento anterior, que para poder hacer un análisis de los datos, cargamos las fechas en columnas de añoactivo Comienzo y Fin, algúnas columnas quedarón sin año FIN, les guardamos el valor None, para ahora reemplazarlas por el valor de la columna COMIENZO."""

import numpy as np
# Llenar NaN en añoactivo_fin con el valor de añoactivo_comienzo si cumple la condición de añoactivo_fin nula
MV_df["añoactivo_fin"] = np.where(MV_df["añoactivo_fin"] == 0, MV_df["añoactivo_comienzo"], MV_df["añoactivo_fin"])

"""Convertimos los datos de la columna "Price" para poder utilizarnos en nuestro análisis"""

# Reemplazar "Free" por 0 y convertir los valores a float en la columna "Price"
def convertir_precios(value):
    if value.strip() == "Free":
        return 0
    elif value.strip() == "None":
        return None
    else:
        cleaned_value = value.replace("$", "").strip()
        return float(cleaned_value)

MV_df["Price"] = MV_df["Price"].apply(convertir_precios)

import pandas as pd
from datetime import datetime

# Tu DataFrame MV_df

# Convertir la columna "publish_date" a formato de fecha
MV_df["publish_date"] = pd.to_datetime(MV_df["publish_date"], format="%B %d, %Y", errors='coerce')

# Función para calcular la diferencia en días entre dos fechas
def calcular_dias(row):
    if not pd.isnull(row["añoactivo_comienzo"]) and row["añoactivo_comienzo"] != 0:
        año_comienzo = int(row["añoactivo_comienzo"])
        return abs((datetime(año_comienzo, 1, 1) - row["publish_date"]).days)
    else:
        return None

# Crear la columna "días" con la diferencia en días
MV_df["días"] = MV_df.apply(calcular_dias, axis=1)

# Eliminar las filas con añoactivo_comienzo igual a 0
MV_df = MV_df[MV_df["añoactivo_comienzo"] != 0]

"""Explicación del analisis y columnas:

"publish_date": tiene los datos en formato fecha de cuando se publico el comic.

"añoactivo_comienzo": tiene los datos en formato año de cuando comenzó a escribirse.

"añoactivo_fin": tiene los datos en formato año de cuando fue publicado.

"días" : Tiene los datos de cuantós días tardo en publicarse desde su inicio de proyecto
"""

#Librerias
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

#Valores atípicos

#Aca vamos a limpiar de valores NONE para la columna dias para poder trabajar con los valores atipicos
MV_df_limpio = MV_df.dropna(subset=['días'])
Mv_df_limpio = MV_df.dropna(subset=['añoactivo_comienzo'])
Mv_df_limpio = MV_df.dropna(subset=['añoactivo_fin'])
Mv_df_limpio = MV_df.dropna(subset=['Price'])

# Seleccionar las columnas para generar los boxplots
columns_to_plot = ["Price", "añoactivo_comienzo", "añoactivo_fin", "días"]

# Crear una figura y un conjunto de ejes para los boxplots
plt.figure(figsize=(12, 6))
plt.title("Boxplots para las columnas seleccionadas")

# Generar los boxplots para cada columna en el ciclo
for column in columns_to_plot:
    plt.subplot(1, len(columns_to_plot), columns_to_plot.index(column) + 1)
    sns.boxplot(data=Mv_df_limpio, y=column)
    plt.ylabel(column)

plt.tight_layout()
plt.show()

"""ANÁLISIS DEL BOXPLOT Y LOS VALORES ATÍPICOS.

-BOXPLOT

Columna "añoactivo_comienzo": se puede observar el bigote inferior contiene el 25% del total de los datos con una alta dispersión, la concentración del 50% de los datos se encuentra dispersa entre 1980-2010, la mediana parecería de 2005, el bigote superior tiene el 25% de los datos pocos dispersos.

Columna "añoactivo_fin": se puede observar varios outliers, parece haber una dispersión de los valores en el bigote superior (25% del total) y el bigote inferior no tan dispersa pero no tan concentrada. El 50% de los datos se encuentran más concentrados, parecería un rango entre 1996-2010, su mediana parecería tener un valor de 2006.

Columna "días": Se puede observar que el 25% de los datos del bigote inferior están MUY concentrados, el 50% del total representado por la caja está altamente concentrado, se dificulta calcular la mediana por los valores de la escala en el eje "Y", el 25% del total del bigote superior se encuentra más disperso que los valores la caja y el bigote inferior

Columna "Precio": No se puede ver el 25% de los valores pertenecientes al bigote inferior. Se puede observar el 50% perteneciente a la caja, muy concentrados y comenzando en el valor 0, parecería abarcar el rango de valores del 0-3. El bigote superior que contiene el 25% de los valores están concentrados. la mediana parecería ser de 0.5 o 1.5.

-VALORES ATÍPICOS

Los valores atípicos en la columna "días" deberían ser tratados, parecen ser errores de entrada o que hubo un caso atípico en la emisión del Comic, pero analizando el dataset son casos en los que los Comics estuvieron en emisión mucho tiempo, a traves del tiempo. Estos van a ser tratados en su mayoría

Los valores atípicos en la columna "Price" son debidos a que no es 1 comic como los otros, estos son comics de tapadura (hardcover) o colección anual o novelas gráficas(graphic novel) o colección de historias(trade paperback) , por lo que su precio es mayor al resto. Estos no se tratarán.

Los valores atípicos en la columna "añoactivo_fin": Están relacionados a que hay una relación entre la cantidad de comics y los años transcurridos en el tiempo. Esto lo explicaremos más adelante, Estos se tratarán porque presenta valores fuera del rango de la fecha actual, pueden emitirse en el futuro pero no se emitieron todavía. Para nuestro análisis, tendremos en cuenta los publicados.

Esto lo demostraremos con el análisis en los siguientes códigos.
"""

print(Mv_df_limpio[Mv_df_limpio["días"]>20000])
print(Mv_df_limpio[Mv_df_limpio["Price"]<99])
print(Mv_df_limpio[Mv_df_limpio["añoactivo_fin"]<1960])

# Identificar atípicos por el método de cuartiles en días
Q1_dias = MV_df_limpio['días'].quantile(0.25)
Q3_dias = MV_df_limpio['días'].quantile(0.75)
IQR_dias = Q3_dias - Q1_dias
lower_bound_dias = Q1_dias - 1.5 * IQR_dias
upper_bound_dias = Q3_dias + 1.5 * IQR_dias

MV_df_limpio2 = MV_df_limpio[(MV_df_limpio['días'] >= lower_bound_dias) & (MV_df_limpio['días'] <= upper_bound_dias)]

# Identificar atípicos por el método de cuartiles en añoactivo_fin
Q1_añoactivo_fin = MV_df_limpio2['añoactivo_fin'].quantile(0.25)
Q3_añoactivo_fin = MV_df_limpio2['añoactivo_fin'].quantile(0.75)
IQR_añoactivo_fin = Q3_añoactivo_fin - Q1_añoactivo_fin
lower_bound_añoactivo_fin = Q1_añoactivo_fin - 1.3 * IQR_añoactivo_fin
upper_bound_añoactivo_fin = Q3_añoactivo_fin + 1.3 * IQR_añoactivo_fin

MV_df_limpio_final = MV_df_limpio2[(MV_df_limpio2['añoactivo_fin'] >= lower_bound_añoactivo_fin) & (MV_df_limpio2['añoactivo_fin'] <= upper_bound_añoactivo_fin)]

import matplotlib.pyplot as plt
import seaborn as sns

# Seleccionar las columnas para generar los boxplots
columns_to_plot = ["Price", "añoactivo_comienzo", "añoactivo_fin", "días"]

# Crear una figura y un conjunto de ejes para los boxplots
plt.figure(figsize=(12, 6))
plt.title("Boxplots para las columnas seleccionadas")

# Generar los boxplots para cada columna en el ciclo
for column in columns_to_plot:
    plt.subplot(1, len(columns_to_plot), columns_to_plot.index(column) + 1)
    sns.boxplot(data=MV_df_limpio_final, y=column)
    plt.ylabel(column)

plt.tight_layout()
plt.show()

"""El añoactivo_fin fue reducido para estar acorde a la fecha, se redujeron los outliers en la columna de días que estuvo en emisión el comic. Los precios cambiarón porque muchos de estos outliers eran comics de tapadura (hardcover) o colección anual o novelas gráficas(graphic novel) o colección de historias(trade paperback)

ANALISIS UNIVARIADO

Variables categóricas
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Gráfico de barras para la variable categórica "format"
plt.figure(figsize=(10, 6))
sns.countplot(data=MV_df_limpio_final, x='Format')
plt.title('Distribución de Formatos')
plt.xticks(rotation=45)
plt.show()

# Gráfico de barras para la variable categórica "Rating"
plt.figure(figsize=(12, 6))
sns.countplot(data=MV_df_limpio_final, x='Rating', order=MV_df_limpio_final['Rating'].value_counts().index[:8])
plt.title('Distribución de Ratings')
plt.xticks(rotation=90)
plt.show()

# Gráfico de barras para la variable categórica "Imprint"
plt.figure(figsize=(12, 6))
sns.countplot(data=MV_df_limpio_final, x='Imprint', order=MV_df_limpio_final['Imprint'].value_counts().index)
plt.title('Distribución de Imprints')
plt.xticks(rotation=90)
plt.show()

"""La mayor cantidad de formatos son Comics, con más de 25000

La mayor cantidad de calificaciones son None, seguido por Rated T+ con 3500 aprox y seguido por T con 1800 aprox.

La mayor cantidad de editoriales son None, seguida por marvel universe con 8000 en una gran cantidad y seguido por marvel knights con 200 aprox, poca cantidad.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Gráfico de pastel para la variable categórica "Format", Se van a analizar solo las 5 con mayor valor en este gráfico
plt.figure(figsize=(8, 8))
format_counts = MV_df_limpio_final['Format'].value_counts()[:5]
plt.pie(format_counts, labels=format_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribución de Formatos')
plt.show()


# Gráfico de pastel para la variable categórica "Rating", Se van a analizar solo las 8 con mayor valor en este gráfico
plt.figure(figsize=(8, 8))
rating_counts = MV_df_limpio_final['Rating'].value_counts()[:8]
plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribución de Ratings')
plt.show()

# Gráfico de pastel para la variable categórica "Imprint", Se van a analizar solo las 7 con mayor valor en este gráfico
plt.figure(figsize=(10, 8))
imprint_counts = MV_df_limpio_final['Imprint'].value_counts()[:7]
plt.pie(imprint_counts, labels=imprint_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribución de Imprints')
plt.show()

"""Gráficos Pie
Formatos: Se analizaron las 5 variabels únicas de mayor valor en este gráfico, Podémos observar que la mayoría de los valores son Comics 93.9%

Ratings : Se analizaron las 8 variables únicas de mayor valor en este gráfico, podémos observar que la mayoría de los valores son comics sin formato 63%, le sigue con un 12.2% rated T+

Imprints: Se analizaron las 7 variables únicas de mayor valor, con un 63.4% ningún sello editorial, luego el 30.5% marvel universe, luego con valores muchos menores marvel knights, ultimate, marvel adventures, max y licensed publishing

Variables numéricas
"""

MV_df_limpio_final.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Gráfico de distribución para la variable numérica "Precio"
plt.figure(figsize=(10, 6))
sns.histplot(data=MV_df_limpio_final, x='Price', bins=20, kde=True)
plt.title('Distribución de Precios')
plt.xlabel('Precio')
plt.ylabel('Frecuencia')
plt.show()

# Gráfico de distribución para la variable numérica "añoactivo_fin"
plt.figure(figsize=(10, 6))
sns.histplot(data=MV_df_limpio_final, x='añoactivo_fin', bins=20, kde=True)
plt.title('Distribución de Años de Finalización')
plt.xlabel('Año de Finalización')
plt.ylabel('Frecuencia')
plt.show()

# Gráfico de distribución para la variable numérica "añoactivo_comienzo"
plt.figure(figsize=(10, 6))
sns.histplot(data=MV_df_limpio_final, x='añoactivo_comienzo', bins=20, kde=True)
plt.title('Distribución de Años de Comienzo')
plt.xlabel('Año de Comienzo')
plt.ylabel('Frecuencia')
plt.show()

# Gráfico de distribución para la variable numérica "días"
plt.figure(figsize=(10, 6))
sns.histplot(data=MV_df_limpio_final, x='días', bins=20, kde=True)
plt.title('Distribución de Días')
plt.xlabel('Días')
plt.ylabel('Frecuencia')
plt.show()

"""Precio: Podemos notar una concentración de los datos entre 0-, su mayor concentración de datos es en el 0. Su distribución de valores no está dispersos y no presenta mucha variabilidad.

Días: Su distribución se encuentra sesgada a la derecha podemos notar una concentración mayor en el rango 250 días, a partir de este rango su frecuencia va disminuyendo gradualmente a mayor cantidad de días.

añoactivo_comienzo: Su distribución se encuentra sesgada a la izquierda, tiene una concentración de los datos a partir del 2000 - 2020, pero la mayor concentración parece estar en el 2010.

añoactivo_fin: Su distribución se encuentra sesgada a la izquierda, tiene una concentración de los datos a partir del 2006 - 2011

ANALISIS MULTIVARIADO
"""

import pandas as pd
import matplotlib.pyplot as plt


# Agrupar los datos por la columna "Format" y calcular la mediana de los precios
median_prices_by_format = MV_df_limpio_final.groupby("Format")["Price"].median()

# Crear un gráfico de barras para mostrar las medianas de precios por formato
plt.figure(figsize=(10, 6))
median_prices_by_format.plot(kind="bar")
plt.xlabel("Format")
plt.ylabel("Median Price")
plt.title("Median Prices by Format")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Podemos ver los mayores precios por sus categorías de comics, corresponde con lo comentado anteriormente"""

# Contar la cantidad de cómics por año activo final
comic_count_by_year = MV_df_limpio_final["añoactivo_fin"].value_counts()

# Convertir los índices del conteo en valores numéricos
years = comic_count_by_year.index.astype(int)

# Crear un gráfico de dispersión
plt.figure(figsize=(10, 6))
plt.scatter(years, comic_count_by_year.values, alpha=0.7)
plt.xlabel("Año Activo Final")
plt.ylabel("Cantidad de Cómics")
plt.title("Relación entre Año Activo Final y Cantidad de Cómics")
plt.grid(True)
plt.tight_layout()
plt.show()

"""Podémos ver que a travez de los años la cantidad de comics emitidos fue creciendo"""

plt.figure(figsize=(12, 6))
sns.countplot(x="Format", hue="añoactivo_comienzo", data=MV_df_limpio_final)
plt.xticks(rotation=45)
plt.title("Distribución de Años de Inicio por Formato")
plt.legend(title="Año de Inicio")
plt.show()

"""Cuesta identificar todos los datos, pero es seguro que a través del tiempo,el formato Comic, tiene la mayor cantidad y concentración de los datos."""

plt.figure(figsize=(10, 6))
plt.scatter(MV_df_limpio_final["añoactivo_comienzo"], MV_df_limpio_final["añoactivo_fin"])
plt.xlabel("Año de Inicio")
plt.ylabel("Año de Fin")
plt.title("Relación entre Año de Inicio y Año de Fin")
plt.show()

"""Se pueden observar algunos datos fuera de rango, pero son pocos, es difícil ver una relación entre la cantidad de tiempo de emisión de un comic con respecto a su fecha de inicio, parecería no afectar el año de inicio con la duración de este, pero se puede observar que hay mucho comics que se emitieron y su emisión tiene su mismo año o más.

Los comics más viejos tienen una concentración de emisión en el año 1990-2000.

La cantidad de emisiones parece aumentar gradualmente a partir del año 1990.
"""

plt.figure(figsize=(12, 6))
sns.lineplot(x="añoactivo_comienzo", y="Price", data=MV_df_limpio_final)
plt.xticks(rotation=45)
plt.xlabel("Año de Inicio")
plt.ylabel("Precio")
plt.title("Distribución de Precios a lo largo de los Años")
plt.show()

""" En este gráfico podémos observar que a lo largo de diferentes años se puede observar que en hay picos de precios (aprox 1966 1970 1979 y 1982 1995), luego a partir del 2000 hay un pico, una caida y un valor creciente exponencial en los precios con un par de caidas leves en el transcurso hasta llegar al año 2022 o 2021."""

# Solo variables numéricas
X = MV_df_limpio_final[["Price","añoactivo_comienzo","añoactivo_fin","días"]].dropna()
Y = MV_df_limpio_final.Format
# Matriz de correlación entre variables numéricas
corr_matrix = X.corr()

# Visualizar la matriz de correlación con un mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correlación')
plt.show()

"""Analizando la matríz de correlación con 2 componentes principales (2 dimensiones) podríamos explicar el 89% de los datos, 0.89 cantidad de valores relacionados en la 2da fila (añoactivo_comienzo) 3er columna (añoactivo_fin). Con la columna "Precio" y "añoactivo_comienzo" explicaríamos el 89% de los valores en nuestro dataframe


analizando la matriz vemos que en

la columna precio, hay una relación fuerte con añoactivo_comienzo (0.65) 65% y una relación fuerte con añoactivo_fin (0.64) 64%, hay una relación dévil con días 0.3

la columna añoactivo_comienzo, tiene una relación fuerte con precio 0.65 ,añoactivo_fin 0.89 y con días 0.58

la columna añoactivo_fin tiene una relación fuerte con precio y con añoactivo_comienzo, pero una relación débil con días.

la columna días tiene una relación fuerte con añoactivo_comienzo y relaciones débiles con añoactivo_fin y Precio.
"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

#Estandarizamos el dataframe X
scaler = StandardScaler()
data_scaled = scaler.fit_transform(X) #Estandarizamos

columns_MV = ["Price","añoactivo_comienzo","añoactivo_fin","días"]
df_scaled = pd.DataFrame(data_scaled, columns=columns_MV)

# Aplicamos el Análisis de Componente Principal (PCA)

pca = PCA()
objeto = pca.fit(data_scaled)

# Desviación estándar de cada componente principal.
print(objeto.singular_values_)
# Varianza explicada por cada CP
print(objeto.explained_variance_)
# Cantidad de variables
print(np.sum(objeto.explained_variance_))

# Varianza acumulada de cada componente principal
varianza_acumulada = np.cumsum(objeto.explained_variance_ratio_)
print("Varianza Acumulada de cada Componente Principal:")
print(varianza_acumulada)

explained_variance=pca.explained_variance_ratio_

plt.figure(figsize=(6, 4))
plt.bar(range(1, len(varianza_acumulada) + 1), explained_variance, alpha=0.5, align='center',label='individual explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')
plt.tight_layout()
# Gráfico de la Varianza Acumulada
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(varianza_acumulada) + 1), varianza_acumulada, marker='o')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Acumulada')
plt.title('Varianza Acumulada de Componentes Principales')
plt.xticks(range(1, len(varianza_acumulada) + 1))
plt.grid(True)
plt.show()

"""Podémos analizar en estos gráficos que en el primer componente ya tenemos un 68% o 69%, con la mayor concentración de explicación de la varianza, seguiendole el segundo componente llegamos al 89% de varianza acumulada.
La relación vista en la matriz de correlación se mantiene, pero es más notoria en el gráfico.

Se puede reducir a 2 dimensiones. Precio y añoactivo_comienzo
"""

# Matriz de Componentes Principales
componentes_matrix = pd.DataFrame(data=objeto.components_, columns=X.columns)
print("Matriz de Componentes Principales:")
print(componentes_matrix)

# Visualización de la Matriz de Componentes
plt.figure(figsize=(12, 6))
sns.heatmap(componentes_matrix, annot=True, cmap='coolwarm')
plt.xlabel('Variables Originales')
plt.ylabel('Componentes Principales')
plt.title('Matriz de Componentes Principales')
plt.show()

# Matriz de Componentes Principales para las 2 primeras componentes
componentes_matrix_2 = pd.DataFrame(data=objeto.components_[:2, :], columns=X.columns)
print("Matriz de Componentes Principales (2 primer0s componentes):")
print(componentes_matrix_2)

# Visualización de la Matriz de Componentes para las 2 primeras componentes
plt.figure(figsize=(12, 6))
sns.heatmap(componentes_matrix_2, annot=True, cmap='coolwarm')
plt.xlabel('Variables Originales')
plt.ylabel('Componentes Principales')
plt.title('Matriz de Componentes Principales (2 primeros componentes)')
plt.show()

"""Solo necesitaremos estos 2 componentes Precio y añoactivo_comienzo"""

# Transformación del dataset utilizando las 2 primeras componentes principales
num_components = 2
data_transformed = pca.transform(data_scaled)[:, :num_components]

# Crear un nuevo DataFrame con las dos primeros componentes principales
columns = [f'PC{i+1}' for i in range(num_components)]
df_transformed = pd.DataFrame(data=data_transformed, columns=columns)
result = pd.concat([df_transformed, Y], axis=1)

# Mostrar el DataFrame transformado
print("Dataset Transformado utilizando los 2 primeros componentes principales:")
result.head()

# Crear el gráfico utilizando Seaborn
# Definir colores para cada variedad
color_palette = sns.color_palette("husl", n_colors=len(result['Format'].unique()))
plt.figure(figsize=(10, 6))
sns.scatterplot(data=result, x='PC1', y='PC2', hue='Format', palette=color_palette)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA: Componentes principales PC1 vs PC2')
plt.legend()
plt.show()

"""

Podemos observar una gran concentración del formato comics, podémos analizar que hay PC1 que sin importar los PC2 (estos posiblemente son nuestros comics de cualquier formato que tienen el precio 0 en el dataframe).

Se puede observar que los PC1 tienden a ser mayores con los PC2 del rango 0-4 y en PC1, hay un pequeño pico rango [ -1 ] en comparación PC2 anteriores del rango [ -2 ], luego en PC1 rango[ 2 - 3 ] comienzan a escalar los PC2."""

# Crear un gráfico de densidad para las componentes principales
plt.figure(figsize=(10, 6))
sns.kdeplot(data=result, x='PC1', y='PC2', fill=True, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Gráfico de Densidad de las Componentes Principales PC1 vs PC2')
plt.show()

"""Podémos obser que la mayor concentración de todos los valores se encuentran entre

PC1 rango [ 1, -2 ] y PC2 rango[ -0.5 , 0 ]. Aproximadamente
"""

from sklearn.cluster import KMeans

# Seleccionar el número de clústeres
num_clusters = 3

# Inicializar el modelo de K-Means
kmeans = KMeans(n_clusters=num_clusters, random_state=42)

# Ajustar el modelo a los datos transformados
kmeans.fit(df_transformed)

# Agregar las etiquetas de clúster al DataFrame de resultados
df_transformed['Cluster'] = kmeans.labels_

# Crear un scatterplot de las componentes principales coloreado por los clústeres
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_transformed, x='PC1', y='PC2', hue='Cluster', palette='tab10')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Análisis de Clústeres utilizando K-Means')
plt.legend()
plt.show()

"""Podémos ver

-2 clusters, verde y azul, de mismo PC1, que comparten el PC1 a través de diferentes PC2. El azul de valores concentrados en un rango de PC2        
[-2 , 0], el Verde de mayor valores variables en PC1 en el rango de PC2 [ -0.5 , 4]

-un tercer cluster, color naranja, de PC1 más altos a traves de todos los PC2 de los 2 clusteres anteriores, valores que pueden ser muy variables pero en su mayoría se encuentran concentrados en una zona. PC1 rango [1 , -2] y PC2 rango[-0.5 , 0]
"""